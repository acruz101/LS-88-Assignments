{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LS 88 - Open Science & Reproducibility: The GRIM Model\n",
    "\n",
    "---\n",
    "\n",
    "### Instructor: Josh Quan & Eric Van Dusen\n",
    "\n",
    "This notebook will cover the basic idea of the GRIM model and explore papers that GRIM overturned. This notebook describes how claims presented in research papers can be tested for validity by simple numerical analysis.\n",
    "\n",
    "**Topics Covered:**\n",
    "1. What is the GRIM Model and how can we use it?\n",
    "2. Examples of papers overturned by the GRIM Model\n",
    "\n",
    "**Table of Contents:**\n",
    "\n",
    "1. [The GRIM Model](#grim) <br>\n",
    "\n",
    "2. [Exploring GRIM](#explore) <br>\n",
    "\n",
    "3. [Applying GRIM to Research](#research) <br>\n",
    "\n",
    "4. [Bibliography](#bibliography) <br>\n",
    "\n",
    "\n",
    "#### Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The GRIM Model <a id='grim'></a>\n",
    "\n",
    "The GRIM test is a method for evaluating the accuracy of published research. GRIM stands for Granularity-Related Inconsistency of Means. What this means is that, when we have a small sample size and granular data (measured in integers), we can check to see if the reported averages are possible based on the sample size. Let's look at an example:\n",
    "\n",
    "Let's say we are measuring the ages of 12 individuals. Their sample ages and the mean of ages are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sample with ages of 12 students\n",
    "ages = make_array(19, 17, 20, 18, 21, 23, 18, 21, 20, 24, 18, 22)\n",
    "age_mean = np.mean(ages)\n",
    "age_mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of GRIM is that we know that the number of people in the dataset is 12, so therefore the `mean * 12` should be an integer because all of the ages were integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12 * age_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With simple numerical analysis, we can tell if researchers were faking their data. Say the original research instead had 15 participants instead of 12 and claimed the mean age was 20.083. Just by multiplying the mean by the sample size, we can see that this mean is not possible given the sample size and nature of the measure -- ages are measured granularly, so the sum of all the ages should be an integer. In this case, we can see that if the researchers did indeed have 15 subjects, the sum of ages would be 301.25. This is an impossible number because no one reported their age using decimals. Thus, we have evidence that the researchers were faking their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15 * age_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the OSF page where you read the GRIM article, there were two Excel files that contained a list of articles and a calculator that determined whether or not a reported mean passed the GRIM test. In this lab, you will be using Python to recreate that calculator. The Excel calcator, when the fractional portion of the reported mean was 0.10 and the number of items was 1, looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read_table('grim_calculator.csv').drop(4, 5, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring GRIM<a id='explore'></a>\n",
    "\n",
    "Below we will explore the idea of GRIM by recreating a GRIM calculator published with the original research. The original spreadsheet took in the decimal portion of the reported mean and the number of items in the scale (this is typically 1). It then calculated the possible means with the given sample size, and highlighted the consistent means. In this example, we will instead create a table of the correct means, allowing for a small window to account for rounding errors when results are published. First, we will create a table that shows means for all sample sizes, then we will create a table that only shows sample sizes where the mean is correct (accounting for some rounding error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 1  # number of items in the scale (typically 1)\n",
    "\n",
    "fractional_part = 0.85  # the fractional part of the mean\n",
    "\n",
    "sample_sizes = np.arange(1, 101) # creates an array of sample sizes from 1 to 100\n",
    "\n",
    "unrounded = fractional_part * sample_sizes  # first step in calculating mean\n",
    "\n",
    "rounded = np.round(unrounded*num_items, 0)/num_items  # rounding to a whole number\n",
    "\n",
    "mean = np.round(rounded / sample_sizes, 3)  # calculating the possible last digits of the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this table shows all sample sizes with both correct and incorrect means\n",
    "grim_calc = Table().with_columns('N', sample_sizes, 'unrounded', unrounded, 'rounded', rounded, 'mean', mean)\n",
    "grim_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you will run across data that you think was reported incorrectly because it doesn't pass the GRIM test, but in reality it's possible that there is just some rounding error and it does pass. The code below creates a table with data for sample sizes where the mean is correct when accounting for some rounding error. Keep in mind that this table depends on our `grim_calc` table, which in turn means that it depends on what our number of items and fractional part was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a table with data for sample sizes where the mean is correct, accounting for some rounding error\n",
    "correct = grim_calc.where('mean', are.between_or_equal_to(fractional_part - .004, fractional_part + 0.004))\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had a sample size of 13 and a fractional part of 0.85, you could run the code below to see that indeed the mean passed the GRIM test! We will work to make this testing process easier by implementing a couple of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reported_n = 13\n",
    "correct_n = correct.column('N')\n",
    "reported_n in correct_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Notice how the `grim_calc` and `correct` tables we created above depend on what we set `num_items` and `fractional_part` to. These values may change with different data. Try defining a function that will generate the table we made above, but have arguments for values that we want to pass in. Call this function `grim_table`. It should take as arguments the number of items and fractional part of the reported mean as parameters and return the table with the same four columns as above. Work with others around you if you get stuck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grim_table(num_items, fractional):\n",
    "    \n",
    "    # ANSWER:\n",
    "    \n",
    "    sample_sizes = ...\n",
    "    unrounded = ...\n",
    "    rounded = ...\n",
    "    mean = ...\n",
    "    \n",
    "    return Table().with_columns(\n",
    "        'N', sample_sizes,\n",
    "        'unrounded', unrounded,\n",
    "        'rounded', rounded,\n",
    "        'mean', mean\n",
    "    )\n",
    "\n",
    "    \n",
    "    # END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Now we have a function that generates a table that tells us the acceptable rounded means for all sample sizes. Use this function to define a _new_ function `grim_test` that will return a Boolean as to whether or not the observed mean is consistent. You should consider means that are between +/- 0.004 of the fractional part to be consistent. This function should take the same arguments as `grim_table` with an additional parameter `N`, the sample size.\n",
    "\n",
    "*Hint: You can call `grim_table` inside your `grim_test` function to get correct mean values!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grim_test(num_items, fractional, N):\n",
    "    \n",
    "    \n",
    "    # ANSWER:\n",
    "    \n",
    "    \n",
    "    table = grim_table(num_items, fractional)\n",
    "    table = table.where(....)\n",
    "    return N in table.column('N')\n",
    "      \n",
    "    \n",
    "    \n",
    "    # END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test your function, use the following parameters. The tuple returned should be `(True, False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grim_test(1, .4, 15), grim_test(1, .6, 37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying GRIM to Research <a id='research'></a>\n",
    "\n",
    "Let's look at how GRIM applies to real research articles. Following is an example of a research paper where values pass the GRIM test, as well as an example where the article fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taste Perception**\n",
    "\n",
    "The following research paper by Eskine, Kacinik, and Prinz tested how perception of taste affected moral judgements. The full article can be found [here](https://journals.sagepub.com/doi/pdf/10.1177/0956797611398497). There were 54 participants, with 15 in the bitter condition, 18 in the sweet condition, and 21 in the control condition. Let's test their Mean Taste Rating for people who got a bitter drink and said it was bitter, to see if the mean passes the GRIM test. Use 1 for num_items. We will be using the `grim` function to speed up the process.\n",
    "\n",
    "<img src=\"eskine-pass.jpg\" width=400>\n",
    "\n",
    "Here's a test for the people who received a bitter drink and said it was bitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grim_test(1, .40, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test any of the other values! Try those who were in the water (control) group but said that the taste was sweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here\n",
    "\n",
    "## ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at an article where means don't pass. \n",
    "\n",
    "**Responses to personal positive events**\n",
    "\n",
    "[This article](https://labs.psych.ucsb.edu/gable/shelly/sites/labs.psych.ucsb.edu.gable.shelly/files/pubs/gable_gosnell_et_al._2012.pdf) by Gable and Gosnell et al., investigates responses to positive and negative events between couples. When we test their values for feelings about interactions for daily personal positive and negative events, we see that the values don't pass the GRIM test. Here is an image of the values that don't pass.\n",
    "\n",
    "<img src=\"gable.png\" width=600>\n",
    "\n",
    "Notice how when we test the values for (positive, supported) and (negative, resentment), the values don't pass! Try testing the other values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive, supported\n",
    "grim_test(1, 0.60, 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative, resentment\n",
    "grim_test(1, 0.04, 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Now it's your turn! Try to find and test 2 articles with values that pass and 2 articles with values that fail. This [spreadsheet](https://docs.google.com/spreadsheets/d/1cWR2mMTh1HW2ZpmlHHwF_S-qNa42V9635dbv_xMx6iw/edit?usp=sharing) may help you with ideas of articles to test. Write out your function calls and analyses below! Make sure to list the article and which specific values fail or pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Article 1: Pass** *write your description of the article here including a link to the article, and list the value/values you tested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Article 2: Pass** *write your description of the article here including a link to the article, and list the value/values you tested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Article 3: Fail** *write your description of the article here including a link to the article, and list the value/values you tested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Article 4: Pass**  *write your description of the article here including a link to the article, and list the value/values you tested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bibliography <a id='bibliography'></a>\n",
    "\n",
    "Nicholas Brown, James Heathers - Defining the GRIM model: Original Paper. https://doi.org/10.1177/1948550616673876\n",
    "\n",
    "Nicholas Brown, James Heathers - xls GRIM calculator tool - https://osf.io/qcvx9/\n",
    "\n",
    "\n",
    "James Heathers - Overview of the GRIM model. https://medium.com/@jamesheathers/the-grim-test-a-method-for-evaluating-published-research-9a4e5f05e870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Keilyn Yuzuki with contributions from Christopher Pyles\n",
    "\n",
    "Data Science Modules: https://data.berkeley.edu/education/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
